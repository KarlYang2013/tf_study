{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "tf_data_generate_tfrecord.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarlYang2013/tf_study/blob/master/tf_data_generate_tfrecord.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQpiykTCiC-2",
        "colab_type": "code",
        "colab": {},
        "outputId": "2c857429-cd76-41d4-da98-eaafffde8f90"
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.__version__)\n",
        "print(sys.version_info)\n",
        "for module in mpl, np, pd, sklearn, tf, keras:\n",
        "    print(module.__name__, module.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n",
            "sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\n",
            "matplotlib 3.1.1\n",
            "numpy 1.17.3\n",
            "pandas 0.25.3\n",
            "sklearn 0.21.3\n",
            "tensorflow 2.0.0\n",
            "tensorflow_core.keras 2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NzMbKn_iC-7",
        "colab_type": "code",
        "colab": {},
        "outputId": "605b26fd-9751-48cf-f947-bc103fead466"
      },
      "source": [
        "source_dir = \"./generate_csv/\"\n",
        "\n",
        "def get_filenames_by_prefix(source_dir, prefix_name):\n",
        "    all_files = os.listdir(source_dir)\n",
        "    results = []\n",
        "    for filename in all_files:\n",
        "        if filename.startswith(prefix_name):\n",
        "            results.append(os.path.join(source_dir, filename))\n",
        "    return results\n",
        "\n",
        "train_filenames = get_filenames_by_prefix(source_dir, \"train\")\n",
        "valid_filenames = get_filenames_by_prefix(source_dir, \"valid\")\n",
        "test_filenames = get_filenames_by_prefix(source_dir, \"test\")\n",
        "\n",
        "import pprint\n",
        "pprint.pprint(train_filenames)\n",
        "pprint.pprint(valid_filenames)\n",
        "pprint.pprint(test_filenames)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['./generate_csv/train_13.csv',\n",
            " './generate_csv/train_03.csv',\n",
            " './generate_csv/train_09.csv',\n",
            " './generate_csv/train_19.csv',\n",
            " './generate_csv/train_15.csv',\n",
            " './generate_csv/train_11.csv',\n",
            " './generate_csv/train_08.csv',\n",
            " './generate_csv/train_02.csv',\n",
            " './generate_csv/train_17.csv',\n",
            " './generate_csv/train_01.csv',\n",
            " './generate_csv/train_07.csv',\n",
            " './generate_csv/train_12.csv',\n",
            " './generate_csv/train_16.csv',\n",
            " './generate_csv/train_14.csv',\n",
            " './generate_csv/train_00.csv',\n",
            " './generate_csv/train_10.csv',\n",
            " './generate_csv/train_05.csv',\n",
            " './generate_csv/train_06.csv',\n",
            " './generate_csv/train_04.csv',\n",
            " './generate_csv/train_18.csv']\n",
            "['./generate_csv/valid_08.csv',\n",
            " './generate_csv/valid_00.csv',\n",
            " './generate_csv/valid_07.csv',\n",
            " './generate_csv/valid_09.csv',\n",
            " './generate_csv/valid_03.csv',\n",
            " './generate_csv/valid_02.csv',\n",
            " './generate_csv/valid_06.csv',\n",
            " './generate_csv/valid_01.csv',\n",
            " './generate_csv/valid_04.csv',\n",
            " './generate_csv/valid_05.csv']\n",
            "['./generate_csv/test_01.csv',\n",
            " './generate_csv/test_03.csv',\n",
            " './generate_csv/test_06.csv',\n",
            " './generate_csv/test_08.csv',\n",
            " './generate_csv/test_07.csv',\n",
            " './generate_csv/test_04.csv',\n",
            " './generate_csv/test_05.csv',\n",
            " './generate_csv/test_02.csv',\n",
            " './generate_csv/test_09.csv',\n",
            " './generate_csv/test_00.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFmFptnQiC--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_csv_line(line, n_fields = 9):\n",
        "    defs = [tf.constant(np.nan)] * n_fields\n",
        "    parsed_fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "    x = tf.stack(parsed_fields[0:-1])\n",
        "    y = tf.stack(parsed_fields[-1:])\n",
        "    return x, y\n",
        "\n",
        "def csv_reader_dataset(filenames, n_readers=5,\n",
        "                       batch_size=32, n_parse_threads=5,\n",
        "                       shuffle_buffer_size=10000):\n",
        "    dataset = tf.data.Dataset.list_files(filenames)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
        "        cycle_length = n_readers\n",
        "    )\n",
        "    dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.map(parse_csv_line,\n",
        "                          num_parallel_calls=n_parse_threads)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "batch_size = 32\n",
        "train_set = csv_reader_dataset(train_filenames,\n",
        "                               batch_size = batch_size)\n",
        "valid_set = csv_reader_dataset(valid_filenames,\n",
        "                               batch_size = batch_size)\n",
        "test_set = csv_reader_dataset(test_filenames,\n",
        "                              batch_size = batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghzY6k0AiC_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def serialize_example(x, y):\n",
        "    \"\"\"Converts x, y to tf.train.Example and serialize\"\"\"\n",
        "    input_feautres = tf.train.FloatList(value = x)\n",
        "    label = tf.train.FloatList(value = y)\n",
        "    features = tf.train.Features(\n",
        "        feature = {\n",
        "            \"input_features\": tf.train.Feature(\n",
        "                float_list = input_feautres),\n",
        "            \"label\": tf.train.Feature(float_list = label)\n",
        "        }\n",
        "    )\n",
        "    example = tf.train.Example(features = features)\n",
        "    return example.SerializeToString()\n",
        "\n",
        "def csv_dataset_to_tfrecords(base_filename, dataset,\n",
        "                             n_shards, steps_per_shard,\n",
        "                             compression_type = None):\n",
        "    options = tf.io.TFRecordOptions(\n",
        "        compression_type = compression_type)\n",
        "    all_filenames = []\n",
        "    \n",
        "    for shard_id in range(n_shards):\n",
        "        filename_fullpath = '{}_{:05d}-of-{:05d}'.format(\n",
        "            base_filename, shard_id, n_shards)\n",
        "        with tf.io.TFRecordWriter(filename_fullpath, options) as writer:\n",
        "            for x_batch, y_batch in dataset.skip(shard_id * steps_per_shard).take(steps_per_shard):\n",
        "                for x_example, y_example in zip(x_batch, y_batch):\n",
        "                    writer.write(\n",
        "                        serialize_example(x_example, y_example))\n",
        "        all_filenames.append(filename_fullpath)\n",
        "    return all_filenames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUKgSflriC_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_shards = 20\n",
        "train_steps_per_shard = 11610 // batch_size // n_shards\n",
        "valid_steps_per_shard = 3880 // batch_size // n_shards\n",
        "test_steps_per_shard = 5170 // batch_size // n_shards\n",
        "\n",
        "output_dir = \"generate_tfrecords\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "train_basename = os.path.join(output_dir, \"train\")\n",
        "valid_basename = os.path.join(output_dir, \"valid\")\n",
        "test_basename = os.path.join(output_dir, \"test\")\n",
        "\n",
        "train_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    train_basename, train_set, n_shards, train_steps_per_shard, None)\n",
        "valid_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    valid_basename, valid_set, n_shards, valid_steps_per_shard, None)\n",
        "test_tfrecord_fielnames = csv_dataset_to_tfrecords(\n",
        "    test_basename, test_set, n_shards, test_steps_per_shard, None)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNL-p_GZiC_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_shards = 20\n",
        "train_steps_per_shard = 11610 // batch_size // n_shards\n",
        "valid_steps_per_shard = 3880 // batch_size // n_shards\n",
        "test_steps_per_shard = 5170 // batch_size // n_shards\n",
        "\n",
        "output_dir = \"generate_tfrecords_zip\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "train_basename = os.path.join(output_dir, \"train\")\n",
        "valid_basename = os.path.join(output_dir, \"valid\")\n",
        "test_basename = os.path.join(output_dir, \"test\")\n",
        "\n",
        "train_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    train_basename, train_set, n_shards, train_steps_per_shard,\n",
        "    compression_type = \"GZIP\")\n",
        "valid_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    valid_basename, valid_set, n_shards, valid_steps_per_shard,\n",
        "    compression_type = \"GZIP\")\n",
        "test_tfrecord_fielnames = csv_dataset_to_tfrecords(\n",
        "    test_basename, test_set, n_shards, test_steps_per_shard,\n",
        "    compression_type = \"GZIP\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzkXnJBMiC_H",
        "colab_type": "code",
        "colab": {},
        "outputId": "3329a824-cdd7-43b6-f5c7-45a8936519cd"
      },
      "source": [
        "pprint.pprint(train_tfrecord_filenames)\n",
        "pprint.pprint(valid_tfrecord_filenames)\n",
        "pprint.pprint(test_tfrecord_fielnames)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['generate_tfrecords_zip/train_00000-of-00020',\n",
            " 'generate_tfrecords_zip/train_00001-of-00020',\n",
            " 'generate_tfrecords_zip/train_00002-of-00020',\n",
            " 'generate_tfrecords_zip/train_00003-of-00020',\n",
            " 'generate_tfrecords_zip/train_00004-of-00020',\n",
            " 'generate_tfrecords_zip/train_00005-of-00020',\n",
            " 'generate_tfrecords_zip/train_00006-of-00020',\n",
            " 'generate_tfrecords_zip/train_00007-of-00020',\n",
            " 'generate_tfrecords_zip/train_00008-of-00020',\n",
            " 'generate_tfrecords_zip/train_00009-of-00020',\n",
            " 'generate_tfrecords_zip/train_00010-of-00020',\n",
            " 'generate_tfrecords_zip/train_00011-of-00020',\n",
            " 'generate_tfrecords_zip/train_00012-of-00020',\n",
            " 'generate_tfrecords_zip/train_00013-of-00020',\n",
            " 'generate_tfrecords_zip/train_00014-of-00020',\n",
            " 'generate_tfrecords_zip/train_00015-of-00020',\n",
            " 'generate_tfrecords_zip/train_00016-of-00020',\n",
            " 'generate_tfrecords_zip/train_00017-of-00020',\n",
            " 'generate_tfrecords_zip/train_00018-of-00020',\n",
            " 'generate_tfrecords_zip/train_00019-of-00020']\n",
            "['generate_tfrecords_zip/valid_00000-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00001-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00002-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00003-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00004-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00005-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00006-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00007-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00008-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00009-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00010-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00011-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00012-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00013-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00014-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00015-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00016-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00017-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00018-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00019-of-00020']\n",
            "['generate_tfrecords_zip/test_00000-of-00020',\n",
            " 'generate_tfrecords_zip/test_00001-of-00020',\n",
            " 'generate_tfrecords_zip/test_00002-of-00020',\n",
            " 'generate_tfrecords_zip/test_00003-of-00020',\n",
            " 'generate_tfrecords_zip/test_00004-of-00020',\n",
            " 'generate_tfrecords_zip/test_00005-of-00020',\n",
            " 'generate_tfrecords_zip/test_00006-of-00020',\n",
            " 'generate_tfrecords_zip/test_00007-of-00020',\n",
            " 'generate_tfrecords_zip/test_00008-of-00020',\n",
            " 'generate_tfrecords_zip/test_00009-of-00020',\n",
            " 'generate_tfrecords_zip/test_00010-of-00020',\n",
            " 'generate_tfrecords_zip/test_00011-of-00020',\n",
            " 'generate_tfrecords_zip/test_00012-of-00020',\n",
            " 'generate_tfrecords_zip/test_00013-of-00020',\n",
            " 'generate_tfrecords_zip/test_00014-of-00020',\n",
            " 'generate_tfrecords_zip/test_00015-of-00020',\n",
            " 'generate_tfrecords_zip/test_00016-of-00020',\n",
            " 'generate_tfrecords_zip/test_00017-of-00020',\n",
            " 'generate_tfrecords_zip/test_00018-of-00020',\n",
            " 'generate_tfrecords_zip/test_00019-of-00020']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P5JJBXPiC_K",
        "colab_type": "code",
        "colab": {},
        "outputId": "b4858920-a429-4279-d7ff-f732e4aa01a4"
      },
      "source": [
        "expected_features = {\n",
        "    \"input_features\": tf.io.FixedLenFeature([8], dtype=tf.float32),\n",
        "    \"label\": tf.io.FixedLenFeature([1], dtype=tf.float32)\n",
        "}\n",
        "\n",
        "def parse_example(serialized_example):\n",
        "    example = tf.io.parse_single_example(serialized_example,\n",
        "                                         expected_features)\n",
        "    return example[\"input_features\"], example[\"label\"]\n",
        "\n",
        "def tfrecords_reader_dataset(filenames, n_readers=5,\n",
        "                             batch_size=32, n_parse_threads=5,\n",
        "                             shuffle_buffer_size=10000):\n",
        "    dataset = tf.data.Dataset.list_files(filenames)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filename: tf.data.TFRecordDataset(\n",
        "            filename, compression_type = \"GZIP\"),\n",
        "        cycle_length = n_readers\n",
        "    )\n",
        "    dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.map(parse_example,\n",
        "                          num_parallel_calls=n_parse_threads)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "tfrecords_train = tfrecords_reader_dataset(train_tfrecord_filenames,\n",
        "                                           batch_size = 3)\n",
        "for x_batch, y_batch in tfrecords_train.take(10):\n",
        "    print(x_batch)\n",
        "    print(y_batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-0.902364   -0.7691417  -0.20091914  0.10880736  0.7507385  -0.08213767\n",
            "   1.3978218  -0.9013472 ]\n",
            " [-0.04001952 -1.7303445   0.3000681   0.09866919  1.183514   -0.05759396\n",
            "   0.567329   -0.11237926]\n",
            " [-0.7809836   0.27216142 -0.8818754  -0.35522854  0.33610874  0.2504401\n",
            "  -0.86503774  0.6616083 ]], shape=(3, 8), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[1.434]\n",
            " [1.45 ]\n",
            " [1.28 ]], shape=(3, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-6.2379646e-01  3.5226166e-01  2.0165265e-01  5.5866838e-03\n",
            "  -4.4687924e-01 -4.2026456e-02  2.5175872e+00 -9.3630147e-01]\n",
            " [ 5.9549600e-01  5.1246214e-01  3.1088710e-02 -2.1964614e-01\n",
            "  -4.0877321e-01 -4.8546847e-03 -8.4170932e-01  6.0168666e-01]\n",
            " [ 8.4598297e-01 -1.8905450e+00  3.0823600e-01 -5.6496419e-02\n",
            "   5.1982555e+00  8.4930323e-03 -5.1977670e-01  4.3690220e-01]], shape=(3, 8), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[0.775]\n",
            " [3.428]\n",
            " [2.363]], shape=(3, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.44454306  0.35226166  0.3828868  -0.07468192 -0.62380004 -0.02691609\n",
            "   1.0945519  -1.2708638 ]\n",
            " [-0.7809836   0.99306357 -1.3342534  -0.4096391  -0.79799896 -0.00708994\n",
            "   0.7866164  -1.1510206 ]\n",
            " [ 0.3331267   0.27216142 -0.04035772 -0.0516873  -0.07217004  0.08370184\n",
            "  -0.81838083  0.65162134]], shape=(3, 8), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[2.254]\n",
            " [1.75 ]\n",
            " [1.381]], shape=(3, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.35129648  0.99306357 -0.5322858  -0.05909695  0.17461179 -0.15094851\n",
            "  -0.87436914  0.71154296]\n",
            " [-0.6793182   0.03186071 -0.8141271  -0.19974409 -0.17469338 -0.04857073\n",
            "  -1.401592    1.2358571 ]\n",
            " [-1.5523194   0.75276285 -0.43924466 -0.47694936 -1.2371255  -0.1615357\n",
            "   1.3464992  -0.91133416]], shape=(3, 8), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[3.398]\n",
            " [1.347]\n",
            " [0.675]], shape=(3, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-1.0287532   1.3935647  -0.17143181 -0.18732804 -0.82158846 -0.1198566\n",
            "   1.7337514  -1.0611382 ]\n",
            " [-0.8882971   1.3134645  -0.7162778   0.02862623  0.01583672  0.00926678\n",
            "  -0.7437298   0.62166053]\n",
            " [-0.4159898  -0.12833975  0.16314358 -0.18686584 -0.6337802   0.02088821\n",
            "   0.7866164  -0.7265758 ]], shape=(3, 8), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[0.639]\n",
            " [1.713]\n",
            " [1.06 ]], shape=(3, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-0.63994145  0.27216142 -0.302529   -0.26436356 -0.51674026 -0.00437533\n",
            "   0.3247131  -0.04247071]\n",
            " [-1.1698486   0.27216142 -0.44664204 -0.12129191  0.1809628   0.10901555\n",
            "   0.1940738   0.21718962]\n",
            " [-0.35125002 -0.7691417  -0.30272537 -0.29820734  0.38782406  0.05769372\n",
            "   0.5999888  -1.1060793 ]], shape=(3, 8), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[0.597]\n",
            " [0.415]\n",
            " [1.891]], shape=(3, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-0.6033888  -0.28854024  0.07851043  0.01287575 -0.10664692  0.12685677\n",
            "  -0.7623926   1.086053  ]\n",
            " [-1.4359477   1.5537652  -0.885842    0.19742827 -0.4559521  -0.03831261\n",
            "   1.3558306  -0.9462884 ]\n",
            " [ 0.6237364  -0.52884096  0.6173578  -0.1478001  -0.25362727  0.00730705\n",
            "  -0.3564776  -0.42696774]], shape=(3, 8), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[0.878]\n",
            " [0.654]\n",
            " [1.586]], shape=(3, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-0.1149365   0.6726626  -0.04296771 -0.00271783 -0.8769329  -0.07191245\n",
            "   1.0992177  -1.2458965 ]\n",
            " [ 1.3858008  -1.0895426   0.67543155 -0.10056756 -0.15110394 -0.01931893\n",
            "  -0.846375    0.9062882 ]\n",
            " [ 0.33595076  1.8741661  -0.37890738 -0.13576226 -1.0447809  -0.18598743\n",
            "   1.0059038  -1.4206679 ]], shape=(3, 8), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[1.563  ]\n",
            " [3.007  ]\n",
            " [5.00001]], shape=(3, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-0.91877544  0.4323619  -0.58472663 -0.05638608  0.03488972  0.0570804\n",
            "  -1.3689322   1.2158833 ]\n",
            " [-1.5252513   0.99306357 -0.597722   -0.10005327 -0.49315083 -0.12236544\n",
            "   1.0898862  -0.8414256 ]\n",
            " [ 0.12755793 -0.12833975 -0.11987112  0.07088655 -0.13023636 -0.11082482\n",
            "   0.87526447 -1.3507593 ]], shape=(3, 8), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[1.394]\n",
            " [0.542]\n",
            " [3.978]], shape=(3, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-0.9788795  -0.68904144 -0.5428887   0.30396038  0.38056576  0.02370696\n",
            "  -1.3502694   1.2358571 ]\n",
            " [-0.882649    0.91296333 -0.44648024 -0.18736154 -0.6201709   0.0041722\n",
            "  -0.7250671   1.1359878 ]\n",
            " [-0.554048    1.3935647  -0.49468088 -0.13191015 -0.2917333  -0.12816294\n",
            "  -0.17451568 -0.5418175 ]], shape=(3, 8), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[0.992]\n",
            " [0.752]\n",
            " [1.926]], shape=(3, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMuL47kjiC_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "tfrecords_train_set = tfrecords_reader_dataset(\n",
        "    train_tfrecord_filenames, batch_size = batch_size)\n",
        "tfrecords_valid_set = tfrecords_reader_dataset(\n",
        "    valid_tfrecord_filenames, batch_size = batch_size)\n",
        "tfrecords_test_set = tfrecords_reader_dataset(\n",
        "    test_tfrecord_fielnames, batch_size = batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z742qZ1biC_O",
        "colab_type": "code",
        "colab": {},
        "outputId": "1dbeb7af-afd6-4da3-e4d5-a12208e307e4"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation='relu',\n",
        "                       input_shape=[8]),\n",
        "    keras.layers.Dense(1),\n",
        "])\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
        "callbacks = [keras.callbacks.EarlyStopping(\n",
        "    patience=5, min_delta=1e-2)]\n",
        "\n",
        "history = model.fit(tfrecords_train_set,\n",
        "                    validation_data = tfrecords_valid_set,\n",
        "                    steps_per_epoch = 11160 // batch_size,\n",
        "                    validation_steps = 3870 // batch_size,\n",
        "                    epochs = 100,\n",
        "                    callbacks = callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 348 steps, validate for 120 steps\n",
            "Epoch 1/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.9708 - val_loss: 0.5879\n",
            "Epoch 2/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4927 - val_loss: 0.4968\n",
            "Epoch 3/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4459 - val_loss: 0.4626\n",
            "Epoch 4/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4293 - val_loss: 0.4480\n",
            "Epoch 5/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4059 - val_loss: 0.4372\n",
            "Epoch 6/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3996 - val_loss: 0.4186\n",
            "Epoch 7/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3855 - val_loss: 0.4148\n",
            "Epoch 8/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3831 - val_loss: 0.4039\n",
            "Epoch 9/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3692 - val_loss: 0.3996\n",
            "Epoch 10/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3774 - val_loss: 0.4018\n",
            "Epoch 11/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3678 - val_loss: 0.3993\n",
            "Epoch 12/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3680 - val_loss: 0.3892\n",
            "Epoch 13/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3645 - val_loss: 0.3883\n",
            "Epoch 14/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3612 - val_loss: 0.3958\n",
            "Epoch 15/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3484 - val_loss: 0.3823\n",
            "Epoch 16/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3563 - val_loss: 0.3861\n",
            "Epoch 17/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3601 - val_loss: 0.3771\n",
            "Epoch 18/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3468 - val_loss: 0.3715\n",
            "Epoch 19/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3441 - val_loss: 0.3774\n",
            "Epoch 20/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3568 - val_loss: 0.3727\n",
            "Epoch 21/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3439 - val_loss: 0.3679\n",
            "Epoch 22/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3439 - val_loss: 0.3725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eJMt5tViC_Q",
        "colab_type": "code",
        "colab": {},
        "outputId": "6ce80fc2-8a57-4a58-dba2-19abc6f5dc90"
      },
      "source": [
        "model.evaluate(tfrecords_test_set, steps = 5160 // batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "161/161 [==============================] - 0s 1ms/step - loss: 0.3899\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.38986776496127523"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_zCzoFCiC_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}